# Hyperparameter Optimization (HPO) configuration
# Used by notebooks/hpo_manual_trials.ipynb to submit SweepJobs

experiment_name: "manual-hpo-sweep"
sweep_display_name: "manual-hpo-sweep"

# Primary optimization objective
metric: "f1"                    # e.g., f1, roc_auc, recall, precision, accuracy
mode: "max"                     # "max" or "min"
sampling_algorithm: "random"    # Options: "random", "grid", "bayesian"

budget:
  max_trials: 2                 # Per-model sweep limit (each model gets up to 2 trials)
  max_concurrent: 2             # Per-model parallelism (each sweep can run up to 2 trials at once)

timeouts:
  total_minutes: 180            # Overall sweep timeout (omit for unlimited)
  trial_minutes: 45             # Per-trial timeout (omit for unlimited)

early_stopping:
  enabled: true
  policy: "bandit"              # Currently only "bandit" is supported
  evaluation_interval: 2        # How often (in trials) to evaluate early termination
  slack_factor: 0.1             # Allowance before terminating underperforming trials

search_space:
  model_types: ["rf", "xgboost"]  # Ordered list of models to explore
  rf:
    n_estimators: [100, 200, 300]
    max_depth: [6, 10]
    min_samples_split: [2, 5]
    min_samples_leaf: [1, 2]
  xgboost:
    n_estimators: [100, 200]
    max_depth: [3, 6]
    learning_rate: [0.05, 0.1]
    subsample: [0.6, 0.8, 1.0]
    colsample_bytree: [0.6, 0.8, 1.0]

# Example production-scale settings (uncomment/tweak as needed)
# budget:
#   max_trials: 30
#   max_concurrent: 4
# timeouts:
#   total_minutes: 360
#   trial_minutes: 60
# early_stopping:
#   enabled: true
#   policy: "bandit"
#   evaluation_interval: 3
#   slack_factor: 0.15
