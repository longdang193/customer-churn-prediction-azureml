{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploy Best Model to an Online Endpoint\n",
        "\n",
        "Use this notebook to convert the best trained model into an MLflow artifact, register it with Azure ML, and deploy it to a managed online endpoint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Prerequisites\n",
        "\n",
        "Make sure you have:\n",
        "\n",
        "- Run `run_pipeline.py` (or the pipeline from `main.ipynb`) so that `outputs/model_output/<model_name>_model.pkl` exists locally.\n",
        "- `azure-ai-ml>=1.14.0`, `mlflow`, and `azure-identity` installed in the current environment.\n",
        "- `config.env` populated with your workspace and data asset settings.\n",
        "\n",
        "If you are on a compute instance, these requirements should already be satisfied.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /workspaces/customer-churn-prediction-azureml\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any\n",
        "\n",
        "import joblib\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from dotenv import load_dotenv\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.ai.ml.entities import Model, ManagedOnlineEndpoint, ManagedOnlineDeployment\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "from src.utils import load_azure_config\n",
        "\n",
        "NOTEBOOK_ROOT = Path.cwd().resolve()\n",
        "PROJECT_ROOT = NOTEBOOK_ROOT if (NOTEBOOK_ROOT / \"src\").exists() else NOTEBOOK_ROOT.parent\n",
        "os.chdir(PROJECT_ROOT)\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Using existing MLflow bundle: /workspaces/customer-churn-prediction-azureml/outputs/xgboost_mlflow\n",
            "Model asset name: bank-churn-best-model\n",
            "Endpoint name: churn-endpoint-1763511924\n",
            "Deployment name: blue\n"
          ]
        }
      ],
      "source": [
        "# --- User Inputs -----------------------------------------------------------\n",
        "OUTPUTS_DIR = PROJECT_ROOT / \"outputs\"\n",
        "MODEL_OUTPUT_DIR = OUTPUTS_DIR / \"model_output\"\n",
        "MODEL_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Check for existing MLflow bundles first (preferred - preserves original Python version)\n",
        "MLFLOW_BUNDLE_DIR = None\n",
        "env_mlflow_path = os.getenv(\"AML_MLFLOW_BUNDLE_PATH\")\n",
        "if env_mlflow_path:\n",
        "    MLFLOW_BUNDLE_DIR = Path(env_mlflow_path)\n",
        "elif OUTPUTS_DIR.exists():\n",
        "    mlflow_bundles = sorted(\n",
        "        [d for d in OUTPUTS_DIR.iterdir() if d.is_dir() and d.name.endswith(\"_mlflow\")],\n",
        "        key=lambda p: p.stat().st_mtime if p.exists() else 0,\n",
        "        reverse=True,\n",
        "    )\n",
        "    if mlflow_bundles:\n",
        "        MLFLOW_BUNDLE_DIR = mlflow_bundles[0]\n",
        "\n",
        "# If no MLflow bundle, look for pickle files\n",
        "MODEL_PICKLE_PATH = None\n",
        "if not MLFLOW_BUNDLE_DIR:\n",
        "    env_model_path = os.getenv(\"AML_MODEL_PICKLE_PATH\")\n",
        "    if env_model_path:\n",
        "        MODEL_PICKLE_PATH = Path(env_model_path)\n",
        "    else:\n",
        "        candidate_pkls = []\n",
        "        for search_dir in [MODEL_OUTPUT_DIR, OUTPUTS_DIR]:\n",
        "            if search_dir.exists():\n",
        "                candidate_pkls.extend(search_dir.glob(\"*_model.pkl\"))\n",
        "        \n",
        "        if candidate_pkls:\n",
        "            MODEL_PICKLE_PATH = sorted(candidate_pkls, key=lambda p: p.stat().st_mtime, reverse=True)[0]\n",
        "        else:\n",
        "            available = \"\\n\".join(str(p) for p in MODEL_OUTPUT_DIR.glob(\"*\")) or \"(directory is empty)\"\n",
        "            raise FileNotFoundError(\n",
        "                \"No MLflow bundles (*_mlflow/) or *_model.pkl files were found. \\n\"\n",
        "                \"Run run_pipeline.py locally or download the best-model artifacts from the pipeline run, \"\n",
        "                \"then place them inside outputs/ or set AML_MLFLOW_BUNDLE_PATH/AML_MODEL_PICKLE_PATH.\\n\"\n",
        "                f\"Current directory listing:\\n{available}\"\n",
        "            )\n",
        "\n",
        "# Directory to store the temporary MLflow model artifact (if converting from pickle)\n",
        "MLFLOW_MODEL_DIR = PROJECT_ROOT / \"artifacts\" / \"mlflow_online_model\"\n",
        "MLFLOW_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Names for Azure resources\n",
        "MODEL_NAME = os.getenv(\"AML_DEPLOY_MODEL_NAME\", \"bank-churn-best-model\")\n",
        "ENDPOINT_NAME = os.getenv(\"AML_ONLINE_ENDPOINT_NAME\", f\"churn-endpoint-{int(time.time())}\")\n",
        "DEPLOYMENT_NAME = os.getenv(\"AML_ONLINE_DEPLOYMENT_NAME\", \"blue\")\n",
        "\n",
        "if MLFLOW_BUNDLE_DIR:\n",
        "    print(f\"✓ Using existing MLflow bundle: {MLFLOW_BUNDLE_DIR}\")\n",
        "    MLFLOW_MODEL_DIR = MLFLOW_BUNDLE_DIR\n",
        "else:\n",
        "    print(f\"Using model pickle: {MODEL_PICKLE_PATH}\")\n",
        "    print(f\"MLflow artifact dir: {MLFLOW_MODEL_DIR}\")\n",
        "\n",
        "print(f\"Model asset name: {MODEL_NAME}\")\n",
        "print(f\"Endpoint name: {ENDPOINT_NAME}\")\n",
        "print(f\"Deployment name: {DEPLOYMENT_NAME}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Overriding of current TracerProvider is not allowed\n",
            "Overriding of current LoggerProvider is not allowed\n",
            "Overriding of current MeterProvider is not allowed\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connected to workspace: churn-ml-workspace | resource group: rg-churn-ml-project-2025-11-15\n"
          ]
        }
      ],
      "source": [
        "# Connect to Azure ML workspace\n",
        "load_dotenv(PROJECT_ROOT / \"config.env\")\n",
        "\n",
        "azure_cfg = load_azure_config()\n",
        "\n",
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception:\n",
        "    credential = InteractiveBrowserCredential()\n",
        "\n",
        "ml_client = MLClient(\n",
        "    credential,\n",
        "    subscription_id=azure_cfg[\"subscription_id\"],\n",
        "    resource_group_name=azure_cfg[\"resource_group\"],\n",
        "    workspace_name=azure_cfg[\"workspace_name\"],\n",
        ")\n",
        "print(\n",
        "    f\"Connected to workspace: {ml_client.workspace_name} | \"\n",
        "    f\"resource group: {ml_client.resource_group_name}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32mUploading xgboost_mlflow (0.43 MBs): 100%|██████████| 430941/430941 [00:01<00:00, 309419.14it/s]\n",
            "\u001b[39m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Registered model: bank-churn-best-model:2\n"
          ]
        }
      ],
      "source": [
        "# Register MLflow model asset\n",
        "model_asset = Model(\n",
        "    name=MODEL_NAME,\n",
        "    path=str(MLFLOW_MODEL_DIR),\n",
        "    type=AssetTypes.MLFLOW_MODEL,\n",
        "    description=\"Best churn model exported from training pipeline\",\n",
        ")\n",
        "registered_model = ml_client.models.create_or_update(model_asset)\n",
        "print(f\"Registered model: {registered_model.name}:{registered_model.version}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create or update managed online endpoint\n",
        "# Check if endpoint already exists and delete if in failed state\n",
        "try:\n",
        "    existing_endpoint = ml_client.online_endpoints.get(ENDPOINT_NAME)\n",
        "    if existing_endpoint.provisioning_state in [\"Failed\", \"Canceled\"]:\n",
        "        print(f\"Endpoint {ENDPOINT_NAME} is in {existing_endpoint.provisioning_state} state. Deleting...\")\n",
        "        ml_client.online_endpoints.begin_delete(ENDPOINT_NAME).result()\n",
        "        print(f\"Deleted failed endpoint {ENDPOINT_NAME}\")\n",
        "        time.sleep(5)  # Wait for deletion to propagate\n",
        "except Exception:\n",
        "    # Endpoint doesn't exist or other error - proceed with creation\n",
        "    pass\n",
        "\n",
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name=ENDPOINT_NAME,\n",
        "    auth_mode=\"key\",\n",
        "    description=\"Online endpoint serving the churn model\",\n",
        ")\n",
        "\n",
        "endpoint = ml_client.begin_create_or_update(endpoint).result()\n",
        "print(f\"Endpoint ready: {endpoint.name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Check: endpoint churn-endpoint-1763505617 exists\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "...................................................................................................................................................................Deployment 'blue' is live\n"
          ]
        }
      ],
      "source": [
        "# Deploy the model\n",
        "deployment = ManagedOnlineDeployment(\n",
        "    name=DEPLOYMENT_NAME,\n",
        "    endpoint_name=\"churn-endpoint-1763505617\",\n",
        "    model=registered_model,\n",
        "    instance_type=os.getenv(\"AML_ONLINE_INSTANCE_TYPE\", \"Standard_D2as_v4\"),\n",
        "    instance_count=int(os.getenv(\"AML_ONLINE_INSTANCE_COUNT\", \"1\")),\n",
        ")\n",
        "\n",
        "ml_client.online_deployments.begin_create_or_update(deployment).result()\n",
        "print(f\"Deployment '{DEPLOYMENT_NAME}' is live\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Readonly attribute principal_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\n",
            "Readonly attribute tenant_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Endpoint traffic updated: {'blue': 100}\n"
          ]
        }
      ],
      "source": [
        "# --- Route traffic to the deployment --------------------------------------\n",
        "endpoint.traffic = {DEPLOYMENT_NAME: 100}\n",
        "ml_client.begin_create_or_update(endpoint).result()\n",
        "print(f\"Endpoint traffic updated: {endpoint.traffic}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Invoke the endpoint\n",
        "\n",
        "Create a JSON file with inference data (for example `sample-data.json`) that matches the shape expected by your model. Use the cell below to invoke the endpoint and inspect predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw response: [0]\n"
          ]
        }
      ],
      "source": [
        "REQUEST_FILE = PROJECT_ROOT / \"sample-data.json\"\n",
        "\n",
        "if not REQUEST_FILE.exists():\n",
        "    raise FileNotFoundError(f\"{REQUEST_FILE} not found.\")\n",
        "\n",
        "response = ml_client.online_endpoints.invoke(\n",
        "    endpoint_name=\"churn-endpoint-1763505617\",\n",
        "    deployment_name=DEPLOYMENT_NAME,\n",
        "    request_file=str(REQUEST_FILE),\n",
        ")\n",
        "print(\"Raw response:\", response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. (Optional) Delete the endpoint\n",
        "\n",
        "Managed online endpoints accrue cost while running. Use the following cell to delete it when you're done testing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to delete the managed endpoint when no longer needed\n",
        "# ml_client.online_endpoints.begin_delete(name=ENDPOINT_NAME)\n",
        "# print(f\"Deleted endpoint {ENDPOINT_NAME}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.25"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
