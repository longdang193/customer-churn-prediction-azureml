{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploy Best Model to an Online Endpoint\n",
        "\n",
        "Use this notebook to convert the best trained model into an MLflow artifact, register it with Azure ML, and deploy it to a managed online endpoint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Prerequisites\n",
        "\n",
        "Make sure you have:\n",
        "\n",
        "- Run `run_pipeline.py` (or the pipeline from `main.ipynb`) so that `outputs/model_output/<model_name>_model.pkl` exists locally.\n",
        "- `azure-ai-ml>=1.14.0`, `mlflow`, and `azure-identity` installed in the current environment.\n",
        "- `config.env` populated with your workspace and data asset settings.\n",
        "\n",
        "If you are on a compute instance, these requirements should already be satisfied.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /workspaces/customer-churn-prediction-azureml\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any\n",
        "\n",
        "import joblib\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from dotenv import load_dotenv\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.ai.ml.entities import Model, ManagedOnlineEndpoint, ManagedOnlineDeployment\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "\n",
        "NOTEBOOK_ROOT = Path.cwd().resolve()\n",
        "PROJECT_ROOT = NOTEBOOK_ROOT if (NOTEBOOK_ROOT / \"src\").exists() else NOTEBOOK_ROOT.parent\n",
        "os.chdir(PROJECT_ROOT)\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "No *_model.pkl files were found in outputs/model_output. \nRun run_pipeline.py locally or download the best-model artifacts from the pipeline run, then place them inside outputs/model_output or set AML_MODEL_PICKLE_PATH to the file you want to use.\nCurrent directory listing:\n(directory is empty)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m candidate_pkls:\n\u001b[1;32m     16\u001b[0m         available \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m MODEL_OUTPUT_DIR\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(directory is empty)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 17\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m     18\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo *_model.pkl files were found in outputs/model_output. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun run_pipeline.py locally or download the best-model artifacts from the pipeline run, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthen place them inside outputs/model_output or set AML_MODEL_PICKLE_PATH to the file you want to use.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent directory listing:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mavailable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m         )\n\u001b[1;32m     23\u001b[0m     MODEL_PICKLE_PATH \u001b[38;5;241m=\u001b[39m candidate_pkls[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Directory to store the temporary MLflow model artifact\u001b[39;00m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: No *_model.pkl files were found in outputs/model_output. \nRun run_pipeline.py locally or download the best-model artifacts from the pipeline run, then place them inside outputs/model_output or set AML_MODEL_PICKLE_PATH to the file you want to use.\nCurrent directory listing:\n(directory is empty)"
          ]
        }
      ],
      "source": [
        "# --- User Inputs -----------------------------------------------------------\n",
        "MODEL_OUTPUT_DIR = PROJECT_ROOT / \"outputs\" / \"model_output\"\n",
        "MODEL_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Allow overriding the exact pickle path via env var; otherwise auto-detect the newest *_model.pkl\n",
        "env_model_path = os.getenv(\"AML_MODEL_PICKLE_PATH\")\n",
        "if env_model_path:\n",
        "    MODEL_PICKLE_PATH = Path(env_model_path)\n",
        "else:\n",
        "    candidate_pkls = sorted(\n",
        "        MODEL_OUTPUT_DIR.glob(\"*_model.pkl\"),\n",
        "        key=lambda p: p.stat().st_mtime,\n",
        "        reverse=True,\n",
        "    )\n",
        "    if not candidate_pkls:\n",
        "        available = \"\\n\".join(str(p) for p in MODEL_OUTPUT_DIR.glob(\"*\")) or \"(directory is empty)\"\n",
        "        raise FileNotFoundError(\n",
        "            \"No *_model.pkl files were found in outputs/model_output. \\n\"\n",
        "            \"Run run_pipeline.py locally or download the best-model artifacts from the pipeline run, \"\n",
        "            \"then place them inside outputs/model_output or set AML_MODEL_PICKLE_PATH to the file you want to use.\\n\"\n",
        "            f\"Current directory listing:\\n{available}\"\n",
        "        )\n",
        "    MODEL_PICKLE_PATH = candidate_pkls[0]\n",
        "\n",
        "# Directory to store the temporary MLflow model artifact\n",
        "MLFLOW_MODEL_DIR = PROJECT_ROOT / \"artifacts\" / \"mlflow_online_model\"\n",
        "MLFLOW_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Names for Azure resources\n",
        "MODEL_NAME = os.getenv(\"AML_DEPLOY_MODEL_NAME\", \"bank-churn-best-model\")\n",
        "ENDPOINT_NAME = os.getenv(\"AML_ONLINE_ENDPOINT_NAME\", f\"churn-endpoint-{int(time.time())}\")\n",
        "DEPLOYMENT_NAME = os.getenv(\"AML_ONLINE_DEPLOYMENT_NAME\", \"blue\")\n",
        "\n",
        "print(f\"Using model pickle: {MODEL_PICKLE_PATH}\")\n",
        "print(f\"MLflow artifact dir: {MLFLOW_MODEL_DIR}\")\n",
        "print(f\"Model asset name: {MODEL_NAME}\")\n",
        "print(f\"Endpoint name: {ENDPOINT_NAME}\")\n",
        "print(f\"Deployment name: {DEPLOYMENT_NAME}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "Pickle file not found. Run run_pipeline.py (or pipeline cell in main.ipynb) to generate outputs/model_output/<model>_model.pkl.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# --- Convert pickle -> MLflow model ---------------------------------------\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m MODEL_PICKLE_PATH\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPickle file not found. Run run_pipeline.py (or pipeline cell in main.ipynb) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto generate outputs/model_output/<model>_model.pkl.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m     )\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(MODEL_PICKLE_PATH)\n\u001b[1;32m      9\u001b[0m mlflow\u001b[38;5;241m.\u001b[39msklearn\u001b[38;5;241m.\u001b[39msave_model(model, path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(MLFLOW_MODEL_DIR), serialization_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcloudpickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Pickle file not found. Run run_pipeline.py (or pipeline cell in main.ipynb) to generate outputs/model_output/<model>_model.pkl."
          ]
        }
      ],
      "source": [
        "# --- Convert pickle -> MLflow model ---------------------------------------\n",
        "if not MODEL_PICKLE_PATH.exists():\n",
        "    raise FileNotFoundError(\n",
        "        \"Pickle file not found. Run run_pipeline.py (or pipeline cell in main.ipynb) \"\n",
        "        \"to generate outputs/model_output/<model>_model.pkl.\"\n",
        "    )\n",
        "\n",
        "model = joblib.load(MODEL_PICKLE_PATH)\n",
        "mlflow.sklearn.save_model(model, path=str(MLFLOW_MODEL_DIR), serialization_format=\"cloudpickle\")\n",
        "print(f\"Saved MLflow model artifact to {MLFLOW_MODEL_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Connect to Azure ML workspace ----------------------------------------\n",
        "load_dotenv(PROJECT_ROOT / \"config.env\")\n",
        "\n",
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception:\n",
        "    credential = InteractiveBrowserCredential()\n",
        "\n",
        "ml_client = MLClient.from_config(credential=credential)\n",
        "print(\n",
        "    f\"Connected to workspace: {ml_client.workspace_name} | \"\n",
        "    f\"resource group: {ml_client.resource_group_name}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Register MLflow model asset ------------------------------------------\n",
        "model_asset = Model(\n",
        "    name=MODEL_NAME,\n",
        "    path=str(MLFLOW_MODEL_DIR),\n",
        "    type=AssetTypes.MLFLOW_MODEL,\n",
        "    description=\"Best churn model exported from training pipeline\",\n",
        ")\n",
        "registered_model = ml_client.models.create_or_update(model_asset)\n",
        "print(f\"Registered model: {registered_model.name}:{registered_model.version}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Create or update managed online endpoint -----------------------------\n",
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name=ENDPOINT_NAME,\n",
        "    auth_mode=\"key\",\n",
        "    description=\"Online endpoint serving the churn model\",\n",
        ")\n",
        "\n",
        "endpoint = ml_client.begin_create_or_update(endpoint).result()\n",
        "print(f\"Endpoint ready: {endpoint.name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Deploy the model ------------------------------------------------------\n",
        "deployment = ManagedOnlineDeployment(\n",
        "    name=DEPLOYMENT_NAME,\n",
        "    endpoint_name=ENDPOINT_NAME,\n",
        "    model=registered_model,\n",
        "    instance_type=os.getenv(\"AML_ONLINE_INSTANCE_TYPE\", \"Standard_DS3_v2\"),\n",
        "    instance_count=int(os.getenv(\"AML_ONLINE_INSTANCE_COUNT\", \"1\")),\n",
        ")\n",
        "\n",
        "ml_client.online_deployments.begin_create_or_update(deployment).result()\n",
        "print(f\"Deployment '{DEPLOYMENT_NAME}' is live\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Route traffic to the deployment --------------------------------------\n",
        "endpoint.traffic = {DEPLOYMENT_NAME: 100}\n",
        "ml_client.begin_create_or_update(endpoint).result()\n",
        "print(f\"Endpoint traffic updated: {endpoint.traffic}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Invoke the endpoint\n",
        "\n",
        "Create a JSON file with inference data (for example `sample-data.json`) that matches the shape expected by your model. Use the cell below to invoke the endpoint and inspect predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "REQUEST_FILE = PROJECT_ROOT / \"sample-data.json\"\n",
        "\n",
        "if not REQUEST_FILE.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"{REQUEST_FILE} not found. Create a JSON payload (list of feature dicts) before invoking.\"\n",
        "    )\n",
        "\n",
        "response = ml_client.online_endpoints.invoke(\n",
        "    endpoint_name=ENDPOINT_NAME,\n",
        "    deployment_name=DEPLOYMENT_NAME,\n",
        "    request_file=str(REQUEST_FILE),\n",
        ")\n",
        "print(\"Raw response:\", response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. (Optional) Delete the endpoint\n",
        "\n",
        "Managed online endpoints accrue cost while running. Use the following cell to delete it when you're done testing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to delete the managed endpoint when no longer needed\n",
        "# ml_client.online_endpoints.begin_delete(name=ENDPOINT_NAME)\n",
        "# print(f\"Deleted endpoint {ENDPOINT_NAME}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.25"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
