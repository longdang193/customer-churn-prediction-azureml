{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual HPO Job Runner\n",
    "\n",
    "This notebook submits individual Azure ML command jobs for each hyperparameter trial so you can inspect every run separately (as opposed to a single SweepJob). The structure follows the [`SweepJob` guidance](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters?view=azureml-api-2) but executes the trials one-by-one for maximum transparency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we run from the project root so component paths resolve correctly\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(f\"Changed working directory to: {os.getcwd()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional\n",
    "\n",
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml import Input, MLClient, load_component\n",
    "\n",
    "# Allow importing project utilities\n",
    "import sys\n",
    "PROJECT_ROOT = Path(\"..\")\n",
    "if str(PROJECT_ROOT.resolve()) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT.resolve()))\n",
    "\n",
    "from src.utils import get_data_asset_config, load_azure_config  # noqa: E402\n",
    "from hpo_utils import build_parameter_space, load_hpo_config  # noqa: E402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"../config.env\")\n",
    "\n",
    "azure_cfg = load_azure_config()\n",
    "data_asset_cfg = get_data_asset_config()\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "ml_client = MLClient(\n",
    "    credential,\n",
    "    subscription_id=azure_cfg[\"subscription_id\"],\n",
    "    resource_group_name=azure_cfg[\"resource_group\"],\n",
    "    workspace_name=azure_cfg[\"workspace_name\"],\n",
    ")\n",
    "\n",
    "workspace_url = f\"https://ml.azure.com/?wsid=/subscriptions/{azure_cfg['subscription_id']}/resourcegroups/{azure_cfg['resource_group']}/workspaces/{azure_cfg['workspace_name']}\"\n",
    "print(f\"Connected to workspace '{azure_cfg['workspace_name']}'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_DIR = str((Path.cwd() / \"src\").resolve())\n",
    "DEFAULT_ENV = os.getenv(\"AML_DEFAULT_ENV\", \"azureml:bank-churn-env:1\")\n",
    "DEFAULT_COMPUTE = os.getenv(\"AML_COMPUTE_CLUSTER\", \"cpu-cluster\")\n",
    "\n",
    "hpo_cfg = load_hpo_config()\n",
    "parameter_space = build_parameter_space(hpo_cfg.get(\"search_space\", {}))\n",
    "\n",
    "train_config_path = Path(\"configs/train.yaml\")\n",
    "with train_config_path.open() as f:\n",
    "    train_cfg = yaml.safe_load(f) or {}\n",
    "\n",
    "raw_input = Input(\n",
    "    type=\"uri_folder\",\n",
    "    path=f\"azureml:{data_asset_cfg['data_asset_name']}:{data_asset_cfg['data_asset_version']}\",\n",
    "    mode=\"mount\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command, Output\n",
    "\n",
    "\n",
    "def _extract_uri_from_mapping(data: Optional[Dict]) -> Optional[str]:\n",
    "    if not data:\n",
    "        return None\n",
    "    for key in (\"uri\", \"path\", \"value\"):\n",
    "        value = data.get(key)\n",
    "        if value:\n",
    "            return value\n",
    "    metadata = data.get(\"metadata\") or {}\n",
    "    for key in (\"uri\", \"path\", \"value\"):\n",
    "        value = metadata.get(key)\n",
    "        if value:\n",
    "            return value\n",
    "    return None\n",
    "\n",
    "\n",
    "def stream_job(job_name: str) -> None:\n",
    "    \"\"\"Stream logs for an Azure ML job.\"\"\"\n",
    "    print(f\"Streaming logs for job: {job_name}\")\n",
    "    ml_client.jobs.stream(job_name)\n",
    "\n",
    "\n",
    "def _resolve_output_uri(job_output) -> str:\n",
    "    \"\"\"Best-effort extraction of the backing URI from a job output.\"\"\"\n",
    "    for attr in (\"uri\", \"path\", \"value\"):\n",
    "        value = getattr(job_output, attr, None)\n",
    "        if value:\n",
    "            return value\n",
    "    if hasattr(job_output, \"_to_dict\"):\n",
    "        data = job_output._to_dict() or {}\n",
    "        extracted = _extract_uri_from_mapping(data)\n",
    "        if extracted:\n",
    "            return extracted\n",
    "    if isinstance(job_output, dict):\n",
    "        extracted = _extract_uri_from_mapping(job_output)\n",
    "        if extracted:\n",
    "            return extracted\n",
    "    raise AttributeError(\"Unable to resolve output URI from job output metadata.\")\n",
    "\n",
    "\n",
    "def _get_output_uri(job, output_name: str) -> str:\n",
    "    output = job.outputs[output_name]\n",
    "    try:\n",
    "        return _resolve_output_uri(output)\n",
    "    except AttributeError:\n",
    "        job_dict = job._to_dict() if hasattr(job, \"_to_dict\") else {}\n",
    "        fallback = _extract_uri_from_mapping((job_dict.get(\"outputs\") or {}).get(output_name))\n",
    "        if fallback:\n",
    "            return fallback\n",
    "        raise\n",
    "\n",
    "\n",
    "def _wait_for_job_completion(job_name: str, poll_interval: int = 15):\n",
    "    \"\"\"Poll a job until it finishes and return the refreshed job.\"\"\"\n",
    "    while True:\n",
    "        fresh_job = ml_client.jobs.get(job_name)\n",
    "        status = getattr(fresh_job, \"status\", None)\n",
    "        if status in {\"Completed\", \"Finished\"}:\n",
    "            return fresh_job\n",
    "        if status in {\"Failed\", \"Canceled\"}:\n",
    "            raise RuntimeError(f\"Job {job_name} finished with status {status}\")\n",
    "        time.sleep(poll_interval)\n",
    "\n",
    "\n",
    "def run_data_prep_job(\n",
    "    *, wait_for_completion: bool = True, stream_logs: bool = True, poll_interval: int = 15\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"Submit data prep job and return metadata (always job name, plus URI when ready).\"\"\"\n",
    "    prep_command = command(\n",
    "        code=SRC_DIR,\n",
    "        command=\"python data_prep.py --input ${{inputs.raw_data}} --output ${{outputs.processed_data}}\",\n",
    "        inputs={\"raw_data\": raw_input},\n",
    "        outputs={\"processed_data\": Output(type=\"uri_folder\")},\n",
    "        environment=DEFAULT_ENV,\n",
    "        compute=DEFAULT_COMPUTE,\n",
    "        experiment_name=\"manual-hpo-data-prep\",\n",
    "        display_name=\"manual-hpo-data-prep\",\n",
    "    )\n",
    "    returned_job = ml_client.jobs.create_or_update(prep_command)\n",
    "    result = {\"job_name\": returned_job.name, \"studio_url\": returned_job.studio_url}\n",
    "    print(f\"Data prep job submitted: {returned_job.name} | Studio: {returned_job.studio_url}\")\n",
    "    if not wait_for_completion:\n",
    "        print(\n",
    "            \"Data prep job is running asynchronously; record the job name above and call \"\n",
    "            \"`fetch_processed_data_uri(job_name)` once it finishes to get the output URI.\"\n",
    "        )\n",
    "        return result\n",
    "    if stream_logs:\n",
    "        stream_job(returned_job.name)\n",
    "        completed_job = ml_client.jobs.get(returned_job.name)\n",
    "    else:\n",
    "        completed_job = _wait_for_job_completion(returned_job.name, poll_interval=poll_interval)\n",
    "    processed_uri = _get_output_uri(completed_job, \"processed_data\")\n",
    "    print(f\"Processed data available at: {processed_uri}\")\n",
    "    result[\"processed_data_uri\"] = processed_uri\n",
    "    return result\n",
    "\n",
    "\n",
    "def fetch_processed_data_uri(\n",
    "    job_name: str, output_name: str = \"processed_data\", poll_interval: int = 15\n",
    ") -> str:\n",
    "    \"\"\"Wait for an existing job to finish and return the processed data URI.\"\"\"\n",
    "    completed_job = _wait_for_job_completion(job_name, poll_interval=poll_interval)\n",
    "    processed_uri = _get_output_uri(completed_job, output_name)\n",
    "    print(f\"Processed data for {job_name} available at: {processed_uri}\")\n",
    "    return processed_uri\n",
    "\n",
    "\n",
    "def build_trial_grid(max_trials: Optional[int] = None) -> List[Dict[str, float]]:\n",
    "    \"\"\"Expand the discrete search space into explicit trial configs.\"\"\"\n",
    "    search_space = hpo_cfg.get(\"search_space\", {})\n",
    "    trials: List[Dict[str, float]] = []\n",
    "    for model_name, model_space in search_space.items():\n",
    "        if not model_space:\n",
    "            trials.append({\"model_type\": model_name})\n",
    "            continue\n",
    "        keys = list(model_space.keys())\n",
    "        values = [model_space[key] for key in keys]\n",
    "        for combo in itertools.product(*values):\n",
    "            trial_params = {f\"{model_name}_{key}\": value for key, value in zip(keys, combo)}\n",
    "            trial_params[\"model_type\"] = model_name\n",
    "            trials.append(trial_params)\n",
    "    if max_trials:\n",
    "        trials = trials[:max_trials]\n",
    "    print(f\"Prepared {len(trials)} trial definitions\")\n",
    "    return trials\n",
    "\n",
    "\n",
    "def submit_training_trial(trial_idx: int, processed_uri: str, trial_params: Dict[str, float]):\n",
    "    \"\"\"Submit a single training job with explicit hyperparameters.\"\"\"\n",
    "    cli_overrides = []\n",
    "    for key, value in trial_params.items():\n",
    "        if key == \"model_type\":\n",
    "            continue\n",
    "        if value is not None:\n",
    "            cli_overrides.append(f\"--set {key}={value}\")\n",
    "    override_str = \" \".join(cli_overrides)\n",
    "    override_segment = f\" {override_str}\" if override_str else \"\"\n",
    "    train_command = command(\n",
    "        code=SRC_DIR,\n",
    "        command=(\n",
    "            \"python train.py \"\n",
    "            \"--data ${{inputs.processed_data}} \"\n",
    "            \"--model-artifact-dir ${{outputs.model_output}} \"\n",
    "            f\"--model-type {trial_params['model_type']}\" + override_segment\n",
    "        ),\n",
    "        inputs={\"processed_data\": Input(type=\"uri_folder\", path=processed_uri)},\n",
    "        outputs={\"model_output\": Output(type=\"uri_folder\")},\n",
    "        environment=DEFAULT_ENV,\n",
    "        compute=DEFAULT_COMPUTE,\n",
    "        experiment_name=hpo_cfg.get(\"experiment_name\", \"manual-hpo-trials\"),\n",
    "        display_name=f\"manual-hpo-trial-{trial_idx:03d}-{trial_params['model_type']}\",\n",
    "    )\n",
    "    returned_job = ml_client.jobs.create_or_update(train_command)\n",
    "    print(f\"Trial {trial_idx} submitted: {returned_job.name} | Studio: {returned_job.studio_url}\")\n",
    "    return returned_job.name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PREP_ASYNC = os.getenv(\"AML_DATA_PREP_ASYNC\", \"false\").lower() in {\"true\", \"1\", \"yes\"}\n",
    "\n",
    "data_prep_result = run_data_prep_job(\n",
    "    wait_for_completion=not DATA_PREP_ASYNC,\n",
    "    stream_logs=not DATA_PREP_ASYNC,\n",
    ")\n",
    "\n",
    "data_prep_job_name = data_prep_result[\"job_name\"]\n",
    "processed_data_uri = data_prep_result.get(\"processed_data_uri\")\n",
    "if not processed_data_uri:\n",
    "    print(f\"Waiting for processed data from job {data_prep_job_name}...\")\n",
    "    processed_data_uri = fetch_processed_data_uri(data_prep_job_name)\n",
    "\n",
    "print(f\"Processed data URI: {processed_data_uri}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_data_uri is guaranteed by the previous cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_definitions = build_trial_grid(max_trials=hpo_cfg.get(\"budget\", {}).get(\"max_trials\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_jobs: List[str] = []\n",
    "for idx, trial_params in enumerate(trial_definitions, start=1):\n",
    "    job_name = submit_training_trial(idx, processed_data_uri, trial_params)\n",
    "    trial_jobs.append(job_name)\n",
    "    # Optionally stream logs synchronously per job\n",
    "    # stream_job(job_name)\n",
    "\n",
    "print(\"Submitted trial jobs:\")\n",
    "for job_name in trial_jobs:\n",
    "    print(f\"  - {job_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- Use `stream_job(job_name)` on any of the trials to follow logs live.\n",
    "- After all jobs finish, open each run in Azure ML Studio to compare metrics and hyperparameters.\n",
    "- Optionally rerun `extract_best_params.py` with the best trial's `parent_run_id` (each trial writes its own MLflow run).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
