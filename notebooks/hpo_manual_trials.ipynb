{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization (HPO) - Manual Sweep Trials\n",
    "\n",
    "This notebook orchestrates hyperparameter optimization sweeps for customer churn prediction models.\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. **Setup**: Configure Azure ML client and load HPO configuration\n",
    "2. **Data**: Set training data URI (from previous data prep job or environment variable)\n",
    "3. **Configure Sweeps**: Build sweep jobs per model type from `configs/hpo.yaml`\n",
    "4. **Submit Sweeps**: Submit sweep jobs to Azure ML (or load previous submissions)\n",
    "5. **Analyze Results**: Find best model and export configuration to `configs/train.yaml`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we run from the project root so component paths resolve correctly\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _find_project_root(start_dir: Path) -> Path:\n",
    "    any_markers = {\".git\", \"pyproject.toml\", \"setup.cfg\"}\n",
    "    required_entries = {\"configs\", \"src\", \"notebooks\"}\n",
    "\n",
    "    def _looks_like_root(path: Path) -> bool:\n",
    "        return any((path / marker).exists() for marker in any_markers) and all(\n",
    "            (path / entry).exists() for entry in required_entries\n",
    "        )\n",
    "\n",
    "    for candidate in [start_dir, *start_dir.parents]:\n",
    "        if _looks_like_root(candidate):\n",
    "            return candidate\n",
    "\n",
    "    try:\n",
    "        for child in start_dir.iterdir():\n",
    "            if child.is_dir() and _looks_like_root(child):\n",
    "                return child\n",
    "    except PermissionError:\n",
    "        pass\n",
    "\n",
    "    env_override = os.getenv(\"AML_PROJECT_ROOT\")\n",
    "    if env_override:\n",
    "        return Path(env_override).resolve()\n",
    "\n",
    "    raise RuntimeError(\n",
    "        \"Unable to determine project root. Set AML_PROJECT_ROOT or start inside the repo.\"\n",
    "    )\n",
    "\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd().resolve()\n",
    "PROJECT_ROOT = _find_project_root(NOTEBOOK_DIR)\n",
    "if Path.cwd() != PROJECT_ROOT:\n",
    "    os.chdir(PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml import Input, MLClient\n",
    "\n",
    "# Allow importing project utilities\n",
    "import sys\n",
    "if str(PROJECT_ROOT.resolve()) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT.resolve()))\n",
    "\n",
    "import importlib\n",
    "import hpo_utils  # noqa: E402\n",
    "hpo_utils = importlib.reload(hpo_utils)\n",
    "from azure.ai.ml.sweep import Choice\n",
    "\n",
    "from src.utils import get_data_asset_config, load_azure_config  # noqa: E402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current LoggerProvider is not allowed\n",
      "Overriding of current MeterProvider is not allowed\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(PROJECT_ROOT / \"config.env\")\n",
    "\n",
    "azure_cfg = load_azure_config()\n",
    "data_asset_cfg = get_data_asset_config()\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "ml_client = MLClient(\n",
    "    credential,\n",
    "    subscription_id=azure_cfg[\"subscription_id\"],\n",
    "    resource_group_name=azure_cfg[\"resource_group\"],\n",
    "    workspace_name=azure_cfg[\"workspace_name\"],\n",
    ")\n",
    "\n",
    "workspace_url = f\"https://ml.azure.com/?wsid=/subscriptions/{azure_cfg['subscription_id']}/resourcegroups/{azure_cfg['resource_group']}/workspaces/{azure_cfg['workspace_name']}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Load Azure ML configuration and initialize the ML client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_DIR = str((PROJECT_ROOT / \"src\").resolve())\n",
    "DEFAULT_ENV = os.getenv(\"AML_DEFAULT_ENV\", \"azureml:bank-churn-env:1\")\n",
    "DEFAULT_COMPUTE = os.getenv(\"AML_COMPUTE_CLUSTER\", \"cpu-cluster\")\n",
    "PROCESSED_DATA_DATASTORE = os.getenv(\"AML_PROCESSED_DATA_DATASTORE\", \"workspaceblobstore\")\n",
    "PROCESSED_DATA_PREFIX = os.getenv(\"AML_PROCESSED_DATA_PREFIX\", \"manual-hpo-data\")\n",
    "\n",
    "hpo_cfg = hpo_utils.load_hpo_config()\n",
    "search_space_cfg = hpo_utils.build_parameter_space(hpo_cfg.get(\"search_space\", {}))\n",
    "\n",
    "train_config_path = PROJECT_ROOT / \"configs\" / \"train.yaml\"\n",
    "if not train_config_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Expected training config at {train_config_path}. Ensure you're running inside the repo.\"\n",
    "    )\n",
    "with train_config_path.open() as f:\n",
    "    train_cfg = yaml.safe_load(f) or {}\n",
    "\n",
    "raw_input = Input(\n",
    "    type=\"uri_folder\",\n",
    "    path=f\"azureml:{data_asset_cfg['data_asset_name']}:{data_asset_cfg['data_asset_version']}\",\n",
    "    mode=\"mount\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command, Output\n",
    "from azure.ai.ml.sweep import BanditPolicy\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "_URI_KEYS = (\n",
    "    \"uri\",\n",
    "    \"path\",\n",
    "    \"value\",\n",
    "    \"uri_folder\",\n",
    "    \"uri_file\",\n",
    "    \"asset_uri\",\n",
    "    \"assetUri\",\n",
    "    \"location\",\n",
    ")\n",
    "\n",
    "\n",
    "def _extract_asset_reference(container: Dict[str, Any]) -> Optional[str]:\n",
    "    asset_name = container.get(\"asset_name\") or container.get(\"assetName\")\n",
    "    asset_version = container.get(\"asset_version\") or container.get(\"assetVersion\")\n",
    "    if asset_name and asset_version:\n",
    "        return f\"azureml:{asset_name}:{asset_version}\"\n",
    "    asset_id = container.get(\"asset_id\") or container.get(\"assetId\")\n",
    "    if asset_id:\n",
    "        return asset_id\n",
    "    return None\n",
    "\n",
    "\n",
    "def _extract_uri_from_mapping(data: Optional[Dict[str, Any]]) -> Optional[str]:\n",
    "    if not data:\n",
    "        return None\n",
    "    if isinstance(data, str):\n",
    "        return data\n",
    "    containers = [data, data.get(\"metadata\") or {}]\n",
    "    for container in containers:\n",
    "        for key in _URI_KEYS:\n",
    "            value = container.get(key)\n",
    "            if value:\n",
    "                return value\n",
    "        asset_ref = _extract_asset_reference(container)\n",
    "        if asset_ref:\n",
    "            return asset_ref\n",
    "    return None\n",
    "\n",
    "\n",
    "def stream_job(job_name: str) -> None:\n",
    "    \"\"\"Stream logs for an Azure ML job.\"\"\"\n",
    "    ml_client.jobs.stream(job_name)\n",
    "\n",
    "\n",
    "def _resolve_output_uri(job_output) -> str:\n",
    "    \"\"\"Best-effort extraction of the backing URI from a job output.\"\"\"\n",
    "    if isinstance(job_output, str):\n",
    "        return job_output\n",
    "    for attr in _URI_KEYS:\n",
    "        value = getattr(job_output, attr, None)\n",
    "        if value:\n",
    "            return value\n",
    "    attr_dict = getattr(job_output, \"__dict__\", {}) or {}\n",
    "    asset_ref = _extract_asset_reference(attr_dict)\n",
    "    if asset_ref:\n",
    "        return asset_ref\n",
    "    if hasattr(job_output, \"as_dict\"):\n",
    "        data = job_output.as_dict() or {}\n",
    "        extracted = _extract_uri_from_mapping(data)\n",
    "        if extracted:\n",
    "            return extracted\n",
    "    if hasattr(job_output, \"_to_dict\"):\n",
    "        data = job_output._to_dict() or {}\n",
    "        extracted = _extract_uri_from_mapping(data)\n",
    "        if extracted:\n",
    "            return extracted\n",
    "    if isinstance(job_output, dict):\n",
    "        extracted = _extract_uri_from_mapping(job_output)\n",
    "        if extracted:\n",
    "            return extracted\n",
    "    raise AttributeError(\"Unable to resolve output URI from job output metadata.\")\n",
    "\n",
    "\n",
    "def _get_output_uri(job, output_name: str) -> str:\n",
    "    outputs = getattr(job, \"outputs\", None) or {}\n",
    "    if output_name not in outputs:\n",
    "        raise KeyError(f\"Job {job.name} has no output named '{output_name}'.\")\n",
    "    output = outputs[output_name]\n",
    "    job_dict = job._to_dict() if hasattr(job, \"_to_dict\") else {}\n",
    "    try:\n",
    "        return _resolve_output_uri(output)\n",
    "    except AttributeError:\n",
    "        fallback = _extract_uri_from_mapping((job_dict.get(\"outputs\") or {}).get(output_name) or {})\n",
    "        if fallback:\n",
    "            return fallback\n",
    "        fallback = _extract_uri_from_mapping(\n",
    "            (job_dict.get(\"job_outputs\") or {}).get(output_name) or {}\n",
    "        )\n",
    "        if fallback:\n",
    "            return fallback\n",
    "        artifact_store = os.getenv(\"AML_ARTIFACT_DATASTORE\", \"workspaceartifactstore\")\n",
    "        run_output_path = (\n",
    "            f\"azureml://datastores/{artifact_store}/paths/ExperimentRun/dcid.{job.name}/\"\n",
    "            f\"outputs/{output_name}/\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Unable to locate explicit URI for {job.name}:{output_name}; \"\n",
    "            f\"falling back to {run_output_path}. Consider registering the output as a data asset.\"\n",
    "        )\n",
    "        return run_output_path\n",
    "\n",
    "\n",
    "def _wait_for_job_completion(job_name: str, poll_interval: int = 15):\n",
    "    \"\"\"Poll a job until it finishes and return the refreshed job.\"\"\"\n",
    "    while True:\n",
    "        fresh_job = ml_client.jobs.get(job_name)\n",
    "        status = getattr(fresh_job, \"status\", None)\n",
    "        if status in {\"Completed\", \"Finished\"}:\n",
    "            return fresh_job\n",
    "        if status in {\"Failed\", \"Canceled\"}:\n",
    "            raise RuntimeError(f\"Job {job_name} finished with status {status}\")\n",
    "        time.sleep(poll_interval)\n",
    "\n",
    "\n",
    "def run_data_prep_job(\n",
    "    *, wait_for_completion: bool = True, stream_logs: bool = True, poll_interval: int = 15\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"Submit data prep job and return metadata (always job name, plus URI when ready).\"\"\"\n",
    "    output_subdir = f\"{PROCESSED_DATA_PREFIX}/{int(time.time())}\"\n",
    "    output_uri = f\"azureml://datastores/{PROCESSED_DATA_DATASTORE}/paths/{output_subdir}\"\n",
    "    prep_command = command(\n",
    "        code=SRC_DIR,\n",
    "        command=\"python data_prep.py --input ${{inputs.raw_data}} --output ${{outputs.processed_data}}\",\n",
    "        inputs={\"raw_data\": raw_input},\n",
    "        outputs={\"processed_data\": Output(type=\"uri_folder\", path=output_uri)},\n",
    "        environment=DEFAULT_ENV,\n",
    "        compute=DEFAULT_COMPUTE,\n",
    "        experiment_name=\"manual-hpo-data-prep\",\n",
    "        display_name=\"manual-hpo-data-prep\",\n",
    "    )\n",
    "    returned_job = ml_client.jobs.create_or_update(prep_command)\n",
    "    result = {\"job_name\": returned_job.name, \"studio_url\": returned_job.studio_url}\n",
    "    print(f\"Data prep job submitted: {returned_job.name} | Studio: {returned_job.studio_url}\")\n",
    "    if not wait_for_completion:\n",
    "        print(\n",
    "            \"Data prep job is running asynchronously; record the job name above and call \"\n",
    "            \"`fetch_processed_data_uri(job_name)` once it finishes to get the output URI.\"\n",
    "        )\n",
    "        return result\n",
    "    if stream_logs:\n",
    "        stream_job(returned_job.name)\n",
    "        completed_job = ml_client.jobs.get(returned_job.name)\n",
    "    else:\n",
    "        completed_job = _wait_for_job_completion(returned_job.name, poll_interval=poll_interval)\n",
    "    processed_uri = _get_output_uri(completed_job, \"processed_data\")\n",
    "    result[\"processed_data_uri\"] = processed_uri\n",
    "    return result\n",
    "\n",
    "\n",
    "def fetch_processed_data_uri(\n",
    "    job_name: str, output_name: str = \"processed_data\", poll_interval: int = 15\n",
    ") -> str:\n",
    "    \"\"\"Wait for an existing job to finish and return the processed data URI.\"\"\"\n",
    "    completed_job = _wait_for_job_completion(job_name, poll_interval=poll_interval)\n",
    "    processed_uri = _get_output_uri(completed_job, output_name)\n",
    "    return processed_uri\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Data\n",
    "\n",
    "**Option A**: Run a new data prep job (cell below) to create processed data  \n",
    "**Option B**: Use existing processed data (skip to the training data URI cell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Run a new data preparation job\n",
    "# Uncomment and run this cell to create new processed data for training\n",
    "\n",
    "# DATA_PREP_ASYNC = os.getenv(\"AML_DATA_PREP_ASYNC\", \"false\").lower() in {\"true\", \"1\", \"yes\"}\n",
    "\n",
    "# data_prep_result = run_data_prep_job(\n",
    "#     wait_for_completion=not DATA_PREP_ASYNC,\n",
    "#     stream_logs=not DATA_PREP_ASYNC,\n",
    "# )\n",
    "\n",
    "# data_prep_job_name = data_prep_result[\"job_name\"]\n",
    "# processed_data_uri = data_prep_result.get(\"processed_data_uri\")\n",
    "# if not processed_data_uri:\n",
    "#     print(f\"Waiting for processed data from job {data_prep_job_name}...\")\n",
    "#     processed_data_uri = fetch_processed_data_uri(data_prep_job_name)\n",
    "\n",
    "# print(f\"✓ Processed data URI: {processed_data_uri}\")\n",
    "# print(f\"✓ Set this as your training_data_uri in the cell below, or set:\")\n",
    "# print(f\"  os.environ['AML_PROCESSED_DATA_URI'] = '{processed_data_uri}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Training Data URI\n",
    "\n",
    "Set the training data URI for sweep jobs. This should point to processed data from a data prep job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training data URI for sweep jobs.\n",
    "# \n",
    "# OPTION 1: Set manually (uncomment and edit the line below):\n",
    "training_data_uri = \"azureml:azureml_sleepy_lime_0cgj10sz9w_output_data_processed_data:1\"\n",
    "\n",
    "# OPTION 2: Get from data prep job cell above (if you ran it)\n",
    "\n",
    "# Get from data prep job if available, otherwise use manually set value\n",
    "try:\n",
    "    training_data_uri = processed_data_uri\n",
    "except NameError:\n",
    "    # Not set from data prep job, must be manually set above\n",
    "    try:\n",
    "        training_data_uri\n",
    "    except NameError:\n",
    "        raise RuntimeError(\n",
    "            \"No training data URI available. Either:\\n\"\n",
    "            \"  1. Uncomment and set training_data_uri manually above, or\\n\"\n",
    "            \"  2. Run the data prep job cell above to create processed data\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Sweep Jobs\n",
    "\n",
    "Build sweep job configurations from `configs/hpo.yaml`. Each model type gets its own sweep.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Submit or Load Sweep Jobs\n",
    "\n",
    "**Option A**: Submit new sweep jobs (run the cell below)  \n",
    "**Option B**: Load previous sweep submissions (skip to the next section)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured sweep for logreg:\n",
      "  metric: f1 (Maximize)\n",
      "  sampling: random\n",
      "  limits: max_total_trials=2 | max_concurrent=2\n",
      "  timeouts: total=60 min | trial=20 min\n",
      "  hyperparameters: use_smote, logreg_C, logreg_solver\n",
      "-\n",
      "Configured sweep for rf:\n",
      "  metric: f1 (Maximize)\n",
      "  sampling: random\n",
      "  limits: max_total_trials=2 | max_concurrent=2\n",
      "  timeouts: total=60 min | trial=20 min\n",
      "  hyperparameters: use_smote, rf_n_estimators, rf_max_depth, rf_min_samples_split, rf_min_samples_leaf, rf_max_features\n",
      "-\n",
      "Configured sweep for xgboost:\n",
      "  metric: f1 (Maximize)\n",
      "  sampling: random\n",
      "  limits: max_total_trials=2 | max_concurrent=2\n",
      "  timeouts: total=60 min | trial=20 min\n",
      "  hyperparameters: use_smote, xgboost_n_estimators, xgboost_max_depth, xgboost_learning_rate, xgboost_subsample, xgboost_colsample_bytree\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "# Configure sweep jobs per model (no cross-model mixing).\n",
    "budget_cfg = hpo_cfg.get(\"budget\", {})\n",
    "timeouts_cfg = hpo_cfg.get(\"timeouts\", {})\n",
    "early_cfg = hpo_cfg.get(\"early_stopping\", {})\n",
    "\n",
    "sweep_jobs = {}\n",
    "for model_name in search_space_cfg.get(\"model_types\", []):\n",
    "    model_space = search_space_cfg.get(model_name)\n",
    "    if not model_space:\n",
    "        print(f\"Skipping sweep for {model_name}: no hyperparameters defined in configs/hpo.yaml.\")\n",
    "        continue\n",
    "\n",
    "    command_segments = [\n",
    "        \"python run_sweep_trial.py\",\n",
    "        \"--data ${{inputs.processed_data}}\",\n",
    "        f\"--model-type {model_name}\",\n",
    "        \"--model-artifact-dir ${{outputs.model_output}}\",\n",
    "    ]\n",
    "\n",
    "    base_command_inputs = {\n",
    "        \"processed_data\": Input(type=\"uri_folder\", path=training_data_uri),\n",
    "    }\n",
    "\n",
    "    sweep_search_space = {}\n",
    "    hyperparam_names = []\n",
    "    \n",
    "    # Add training-level parameters (use_smote, class_weight, random_state) if they're in search space\n",
    "    # These are swept across all models, not model-specific\n",
    "    training_level_params = [\"use_smote\", \"class_weight\", \"random_state\"]\n",
    "    for param_name in training_level_params:\n",
    "        if param_name in search_space_cfg:\n",
    "            param_values = search_space_cfg[param_name]\n",
    "            # Only add to sweep if it's a list (for sweeping), not a single value\n",
    "            if isinstance(param_values, list):\n",
    "                hyperparam_names.append(param_name)\n",
    "                command_segments.append(f\"--{param_name} ${{{{search_space.{param_name}}}}}\")\n",
    "                sweep_search_space[param_name] = Choice(values=param_values)\n",
    "    \n",
    "    # Add model-specific hyperparameters\n",
    "    for hp_name, hp_values in model_space.items():\n",
    "        prefixed_name = f\"{model_name}_{hp_name}\"\n",
    "        hyperparam_names.append(prefixed_name)\n",
    "        command_segments.append(f\"--{prefixed_name} ${{{{search_space.{prefixed_name}}}}}\")\n",
    "        sweep_search_space[prefixed_name] = Choice(values=hp_values)\n",
    "\n",
    "    sweep_command = \" \".join(command_segments)\n",
    "\n",
    "    base_training_command = command(\n",
    "        code=SRC_DIR,\n",
    "        command=sweep_command,\n",
    "        inputs=base_command_inputs,\n",
    "        outputs={\"model_output\": Output(type=\"uri_folder\")},\n",
    "        environment=DEFAULT_ENV,\n",
    "        compute=DEFAULT_COMPUTE,\n",
    "        display_name=f\"manual-hpo-sweep-trial-{model_name}\",\n",
    "        experiment_name=hpo_cfg.get(\"experiment_name\", \"manual-hpo-sweep\"),\n",
    "    )\n",
    "\n",
    "    early_policy = None\n",
    "    if early_cfg.get(\"enabled\"):\n",
    "        policy_name = (early_cfg.get(\"policy\", \"bandit\") or \"bandit\").lower()\n",
    "        if policy_name != \"bandit\":\n",
    "            raise ValueError(f\"Unsupported early stopping policy: {policy_name}\")\n",
    "        eval_interval = max(1, int(early_cfg.get(\"evaluation_interval\", 2)))\n",
    "        delay_eval = max(1, int(early_cfg.get(\"delay_evaluation\", eval_interval)))\n",
    "        slack_factor = early_cfg.get(\"slack_factor\")\n",
    "        slack_amount = early_cfg.get(\"slack_amount\")\n",
    "        early_policy = BanditPolicy(\n",
    "            evaluation_interval=eval_interval,\n",
    "            delay_evaluation=delay_eval,\n",
    "            slack_factor=slack_factor,\n",
    "            slack_amount=slack_amount,\n",
    "        )\n",
    "\n",
    "    sweep_kwargs = {\n",
    "        \"primary_metric\": hpo_cfg.get(\"metric\", \"f1\"),\n",
    "        \"goal\": \"Maximize\" if hpo_cfg.get(\"mode\", \"max\").lower() == \"max\" else \"Minimize\",\n",
    "        \"sampling_algorithm\": hpo_cfg.get(\"sampling_algorithm\", \"random\"),\n",
    "        \"search_space\": sweep_search_space,\n",
    "        \"early_termination_policy\": early_policy,\n",
    "    }\n",
    "    if budget_cfg.get(\"max_trials\"):\n",
    "        sweep_kwargs[\"max_total_trials\"] = budget_cfg[\"max_trials\"]\n",
    "    if budget_cfg.get(\"max_concurrent\"):\n",
    "        sweep_kwargs[\"max_concurrent_trials\"] = min(budget_cfg[\"max_concurrent\"], sweep_kwargs.get(\"max_total_trials\", budget_cfg.get(\"max_concurrent\")))\n",
    "    if timeouts_cfg.get(\"total_minutes\"):\n",
    "        sweep_kwargs[\"timeout\"] = int(timeouts_cfg[\"total_minutes\"]) * 60\n",
    "    if timeouts_cfg.get(\"trial_minutes\"):\n",
    "        sweep_kwargs[\"trial_timeout\"] = int(timeouts_cfg[\"trial_minutes\"]) * 60\n",
    "\n",
    "    sweep_job = base_training_command.sweep(**sweep_kwargs)\n",
    "    sweep_job.display_name = f\"{hpo_cfg.get('sweep_display_name', 'manual-hpo-sweep')}-{model_name}\"\n",
    "    sweep_job.experiment_name = hpo_cfg.get(\"experiment_name\", \"manual-hpo-sweep\")\n",
    "    sweep_jobs[model_name] = sweep_job\n",
    "\n",
    "if not sweep_jobs:\n",
    "    raise RuntimeError(\"No sweep jobs were created. Check configs/hpo.yaml::search_space.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Previous Sweep Submissions\n",
    "\n",
    "Load existing sweep jobs by name or auto-discover from the experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sweep job submitted for logreg!\n",
      "  Name      : frank_tent_7zshzmfxnt\n",
      "  Status    : Running\n",
      "  Studio URL: https://ml.azure.com/runs/frank_tent_7zshzmfxnt?wsid=/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourcegroups/rg-churn-ml-project-2025-11-15/workspaces/churn-ml-workspace&tid=e7572e92-7aee-4713-a3c4-ba64888ad45f\n",
      "-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sweep job submitted for rf!\n",
      "  Name      : great_kitchen_qxthwhlfy3\n",
      "  Status    : Running\n",
      "  Studio URL: https://ml.azure.com/runs/great_kitchen_qxthwhlfy3?wsid=/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourcegroups/rg-churn-ml-project-2025-11-15/workspaces/churn-ml-workspace&tid=e7572e92-7aee-4713-a3c4-ba64888ad45f\n",
      "-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sweep job submitted for xgboost!\n",
      "  Name      : musing_salt_8zbzdr8wtr\n",
      "  Status    : Running\n",
      "  Studio URL: https://ml.azure.com/runs/musing_salt_8zbzdr8wtr?wsid=/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourcegroups/rg-churn-ml-project-2025-11-15/workspaces/churn-ml-workspace&tid=e7572e92-7aee-4713-a3c4-ba64888ad45f\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "# Submit new sweep jobs and populate sweep_submissions.\n",
    "# Run this cell to create new sweeps, or skip to load previous submissions below.\n",
    "sweep_submissions = {}\n",
    "for model_name, sweep_job in sweep_jobs.items():\n",
    "    submission = ml_client.jobs.create_or_update(sweep_job)\n",
    "    sweep_submissions[model_name] = submission\n",
    "    print(f\"✓ {model_name}: {submission.name} | {submission.studio_url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Results\n",
    "\n",
    "Find the best model across all sweeps and display its parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_previous_sweeps(ml_client, search_space_cfg: dict, hpo_cfg: dict, specific_sweep_jobs: dict | None = None, auto_discovery: bool | None = None):\n",
    "    \"\"\"Load previous sweep jobs by model name using:\n",
    "       1) Explicit job names (if provided)\n",
    "       2) Auto-discovery (if enabled via parameter or env var)\n",
    "    \"\"\"\n",
    "    specific_sweep_jobs = specific_sweep_jobs or {}\n",
    "    if auto_discovery is None:\n",
    "        use_auto_discovery = os.getenv(\"AML_USE_PREVIOUS_SWEEPS\", \"false\").lower() in {\"true\", \"1\", \"yes\"}\n",
    "    else:\n",
    "        use_auto_discovery = auto_discovery\n",
    "\n",
    "    sweep_submissions = {}\n",
    "    model_types = search_space_cfg.get(\"model_types\", [])\n",
    "\n",
    "    # 1) Explicit job names\n",
    "    for model_name, job_name in specific_sweep_jobs.items():\n",
    "        try:\n",
    "            job = ml_client.jobs.get(job_name)\n",
    "            if job.type == \"sweep\":\n",
    "                sweep_submissions[model_name] = job\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 2) Auto-discovery for missing models\n",
    "    if use_auto_discovery and len(sweep_submissions) < len(model_types):\n",
    "        experiment_name = hpo_cfg.get(\"experiment_name\", \"manual-hpo-sweep\")\n",
    "        prefix = hpo_cfg.get(\"sweep_display_name\", \"manual-hpo-sweep\")\n",
    "\n",
    "        for job in ml_client.jobs.list():\n",
    "            if (\n",
    "                job.type == \"sweep\"\n",
    "                and getattr(job, \"experiment_name\", None) == experiment_name\n",
    "                and getattr(job, \"display_name\", \"\").startswith(prefix)\n",
    "            ):\n",
    "                for model_name in model_types:\n",
    "                    if model_name not in sweep_submissions:\n",
    "                        if f\"-{model_name}\" in job.display_name or job.display_name.endswith(model_name):\n",
    "                            sweep_submissions[model_name] = job\n",
    "\n",
    "    if not sweep_submissions:\n",
    "        print(\"No sweeps found.\")\n",
    "\n",
    "    return sweep_submissions\n",
    "\n",
    "\n",
    "# Example usage\n",
    "SPECIFIC_SWEEP_JOBS = {\n",
    "    # \"rf\": \"gentle_ear_wx2w5x8k5t\",\n",
    "}\n",
    "\n",
    "sweep_submissions = load_previous_sweeps(\n",
    "    ml_client=ml_client,\n",
    "    search_space_cfg=search_space_cfg,\n",
    "    hpo_cfg=hpo_cfg,\n",
    "    specific_sweep_jobs=SPECIFIC_SWEEP_JOBS,\n",
    "    auto_discovery=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Best Model Configuration\n",
    "\n",
    "Export the best model's hyperparameters to `configs/train.yaml` for production training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model:\n",
      "  Model: xgboost\n",
      "  Metric (f1): 0.6362573099415205\n",
      "  Parameters: {'use_smote': True, 'class_weight': 'balanced', 'random_state': 42, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.1, 'subsample': 1, 'colsample_bytree': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# Find and display the best model with its parameters.\n",
    "import ast\n",
    "\n",
    "primary_metric_name = hpo_cfg.get(\"metric\", \"f1\")\n",
    "parameter_coercions = {\"true\": True, \"false\": False, \"none\": None}\n",
    "\n",
    "\n",
    "def _coerce(value):\n",
    "    if isinstance(value, str):\n",
    "        lowered = value.strip().lower()\n",
    "        if lowered in parameter_coercions:\n",
    "            return parameter_coercions[lowered]\n",
    "        try:\n",
    "            return ast.literal_eval(value)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return value\n",
    "    return value\n",
    "\n",
    "\n",
    "best_overall = None\n",
    "\n",
    "for model_name, submission in sweep_submissions.items():\n",
    "    sweep_name = submission.name\n",
    "    sweep_job = ml_client.jobs.get(sweep_name)\n",
    "    best_child_run_id = sweep_job.properties.get(\"best_child_run_id\")\n",
    "    raw_score = sweep_job.properties.get(\"score\")\n",
    "\n",
    "    if not best_child_run_id:\n",
    "        continue\n",
    "\n",
    "    metric_value = None\n",
    "    if raw_score is not None:\n",
    "        try:\n",
    "            metric_value = float(raw_score)\n",
    "        except (TypeError, ValueError):\n",
    "            metric_value = raw_score\n",
    "\n",
    "    if metric_value is None:\n",
    "        continue\n",
    "\n",
    "    child_job = ml_client.jobs.get(best_child_run_id)\n",
    "    params = {k: _coerce(v) for k, v in (getattr(child_job, \"parameters\", {}) or {}).items()}\n",
    "\n",
    "    if best_overall is None or metric_value > best_overall[\"metric\"]:\n",
    "        best_overall = {\n",
    "            \"model_name\": model_name,\n",
    "            \"metric\": metric_value,\n",
    "            \"params\": params,\n",
    "        }\n",
    "\n",
    "if best_overall:\n",
    "    print(f\"Best Model: {best_overall['model_name']} ({primary_metric_name}={best_overall['metric']:.4f})\")\n",
    "    print(f\"Parameters: {best_overall['params']}\")\n",
    "else:\n",
    "    print(\"No completed trials found. Re-run this cell after sweeps finish.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Exported best model configuration to configs/train.yaml\n",
      "  Model: xgboost\n",
      "  Hyperparameters: {'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.1, 'subsample': 1, 'colsample_bytree': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# Export best model configuration to train.yaml\n",
    "if best_overall:\n",
    "    model_name = best_overall[\"model_name\"]\n",
    "    params = best_overall[\"params\"]\n",
    "    \n",
    "    # Load existing config to preserve structure\n",
    "    config_path = Path(\"configs/train.yaml\")\n",
    "    config_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    existing_config = {}\n",
    "    if config_path.exists():\n",
    "        with open(config_path, \"r\") as f:\n",
    "            existing_config = yaml.safe_load(f) or {}\n",
    "    \n",
    "    training_config = existing_config.get(\"training\", {})\n",
    "    \n",
    "    # Extract and normalize parameters\n",
    "    training_level_keys = [\"use_smote\", \"class_weight\", \"random_state\"]\n",
    "    training_configs = {}\n",
    "    model_hyperparams = {}\n",
    "    prefix = f\"{model_name}_\"\n",
    "    \n",
    "    for key, value in params.items():\n",
    "        # Convert booleans to quoted strings\n",
    "        if isinstance(value, bool):\n",
    "            value = \"true\" if value else \"false\"\n",
    "        \n",
    "        if key in training_level_keys:\n",
    "            training_configs[key] = value\n",
    "        elif key.startswith(prefix):\n",
    "            model_hyperparams[key[len(prefix):]] = value\n",
    "        elif not any(key.startswith(f\"{p}_\") for p in [\"rf\", \"logreg\", \"xgboost\"]):\n",
    "            model_hyperparams[key] = value\n",
    "    \n",
    "    # Update config (preserve existing structure)\n",
    "    training_config[\"models\"] = [model_name]\n",
    "    training_config.update(training_configs)\n",
    "    training_config.setdefault(\"hyperparameters\", {})[model_name] = model_hyperparams\n",
    "    \n",
    "    # Force quote all strings in YAML\n",
    "    class QuotedString(str):\n",
    "        pass\n",
    "    \n",
    "    yaml.add_representer(QuotedString, lambda d, v: d.represent_scalar('tag:yaml.org,2002:str', v, style='\"'))\n",
    "    \n",
    "    def quote_strings(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            return {k: quote_strings(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [quote_strings(item) for item in obj]\n",
    "        elif isinstance(obj, str):\n",
    "            return QuotedString(obj)\n",
    "        return obj\n",
    "    \n",
    "    # Write to file\n",
    "    final_config = {**existing_config, \"training\": training_config}\n",
    "    with open(config_path, \"w\") as f:\n",
    "        yaml.dump(quote_strings(final_config), f, default_flow_style=False, sort_keys=False, allow_unicode=True)\n",
    "    \n",
    "    print(f\"✓ Exported to {config_path}\")\n",
    "else:\n",
    "    print(\"No best model found. Run the best model analysis cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Training Pipeline\n",
    "\n",
    "Run the training pipeline with the exported best model configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class DeploymentTemplateOperations: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "\u001b[32mUploading src (0.08 MBs): 100%|██████████| 78248/78248 [00:01<00:00, 45893.22it/s]\n",
      "\u001b[39m\n",
      "\n",
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n",
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFileJobOutput'> and will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Job submitted: witty_lizard_0f8czt39nb\n",
      "  View in Azure ML Studio: https://ml.azure.com/runs/witty_lizard_0f8czt39nb?wsid=/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourcegroups/rg-churn-ml-project-2025-11-15/workspaces/churn-ml-workspace&tid=e7572e92-7aee-4713-a3c4-ba64888ad45f\n"
     ]
    }
   ],
   "source": [
    "# Run the training pipeline with the exported best model configuration\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "if best_overall:\n",
    "    try:\n",
    "        subprocess.run(\n",
    "            [sys.executable, \"run_pipeline.py\"],\n",
    "            cwd=PROJECT_ROOT,\n",
    "            check=True,\n",
    "            capture_output=False,\n",
    "        )\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"✗ Pipeline submission failed with exit code {e.returncode}\")\n",
    "        raise\n",
    "    except FileNotFoundError:\n",
    "        print(\"✗ run_pipeline.py not found. Ensure you're in the project root.\")\n",
    "        raise\n",
    "else:\n",
    "    print(\"No best model found. Run the export cell first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **Monitor Progress**: Use `ml_client.jobs.stream(<job.name>)` or the Studio URL to monitor sweep progress\n",
    "- **Analyze Results**: After sweeps complete, run the analysis cells above to find the best model\n",
    "- **Export Config**: The best model configuration will be exported to `configs/train.yaml` for production training\n",
    "- **Run Pipeline**: Use the pipeline cell above to train the best model, or run `run_pipeline.py` manually\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
